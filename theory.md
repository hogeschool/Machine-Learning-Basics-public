Theorie bestaat uit de volgende onderwerpen

**1. Basic Datastructures and recap** 

- Introduction to Complexity Analysis 

- List, Tuple, Dictionary, Set, Queue, Stack, Linked List 

- Assignments on memory comparison between different data structures (practicum)  


**2.  Graphs and Trees** 

- Graphs, Trees 

- Complexity of these. 

- Search by BFS, DFS 

- Shortest Path algorithms: Dijkstra, A* 

- Building tree representations with different data structures (practicum) 

- Building algorithms to search trees assignments (practicum)   

 

**3. Working with numpy**

- Numpy basics 

- Doing calculations with arrays/matrices 

- Numpy functions 

- Dot product 

- Exercises with numpy and their functionalities (practicum)   

- Exercises exploring data (practicum) 

- Exercises plotting and visualizing data (practicum) 

 

**4. Data representation**

- Categorical vs Continuous data 

- One hot encoding 

- Exercises with one hot encoding using different datastructures (practicum) 

- Exeperiment with other representations of different kinds of data (practicum) 

 
 

**5. Unsupervised learning** 

- Difference between Supervised and Unsupervised learning 

- Examples and applications of Unsupervised learning 

- Principal Component Analysis 

- K-means clustering 

- Exercises with k-means clustering and PCA with different datastructures (practicum) 

 

**6. Supervised Learning** 

- Supervised learning applications 

- Regression vs Classification 

- Dicision trees 

- Linear regression 

- Introduction to deep learning 

- Test/training set explanation 

- KNN algorithm 

- Making predictions with simple supervised algorithms (eg KNN) (practicum) 



 

**8. Neural Networks Introduction**

- Purpose 

- Introduction to global architecture: Layers, weights, bias etc 

- Forward step 

- Data representation 

- Explore MNIST database (practicum) 

- Experiment with test/train set and data representations (practicum) 

 

**9. Architecture and data for the NN** 

- Architecture in more detail, why certain choices are made (eg layers, nodes etc.) 

- Introduction to epochs, accuracy and loss 

- Predictions and Evaluations 

- Explanation of using a library to create a simple NN and how to experiment with it 

- Experiment with architectures. Experiment with different data representations. Create a simple Neural Network with a library (practicum) 

 

**10. Activation functions** 

- Forward propagation 

- activation functions: Relu, Sigmoid, Softmax 

- Implement activation function without external libraries except for numpy: ReLU, Sigmoid, 

- Softmax 

- Start implementing a minimal  NN (without backpropagation): a forward step using activations (practicum) 

- Experiment with different datastructures  (practicum) 

 

**11. Back propagation** 

- Basic explanation and purpose 

- Gradient 

- Derivative of activation functions 

- Cross-entropy loss 

- Loss derivative 

- Learning rate 

- Backpropagation in depth pseudocode 

- Implement a function to calculate loss and experiment. (practicum) 

- Implement a derivative activation function (practicum) 

- Implement a backpropagation function. (practicum) 

 

**12.  Train and predict** 

- Complete and continue working on your NN and data representation with different data structures and representations (practicum) 

- Compare your results and select the best one for the problem at hand 

**13. Optional subjects**

- Transformer

- Attention

- Convolution

- etc

